"""
æµ‹è¯• SFT æ•°æ®é›†çš„ label è®¾ç½®æ˜¯å¦æ­£ç¡®
éªŒè¯ï¼š
1. åªå¯¹ assistant çš„å®é™…å›å¤å†…å®¹è®¡ç®— loss
2. åŒ…å« <|im_end|> ç»“æŸç¬¦
3. æ”¯æŒå¤šè½®å¯¹è¯
"""

import sys
import os

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from transformers import AutoTokenizer
from training.sft import SFTDataset
import json


def test_label_masking():
    """æµ‹è¯• label masking æ˜¯å¦ç¬¦åˆé¢„æœŸ"""
    tokenizer = AutoTokenizer.from_pretrained("./lulu_tokenizer")

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_conversations = [
        {
            "conversations": [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚"},
                {"role": "user", "content": "ä½ å¥½ï¼Œè¯·é—®ä½ çš„åå­—æ˜¯ä»€ä¹ˆï¼Ÿ"},
                {"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ã€‚"},
                {"role": "user", "content": "å¾ˆé«˜å…´è®¤è¯†ä½ ï¼"},
                {
                    "role": "assistant",
                    "content": "æˆ‘ä¹Ÿå¾ˆé«˜å…´è®¤è¯†ä½ ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ",
                },
            ]
        }
    ]

    # ä¿å­˜ä¸´æ—¶æµ‹è¯•æ–‡ä»¶
    test_file = "test_sft_temp.jsonl"
    with open(test_file, "w", encoding="utf-8") as f:
        for conv in test_conversations:
            f.write(json.dumps(conv, ensure_ascii=False) + "\n")

    try:
        # åˆ›å»ºæ•°æ®é›†
        dataset = SFTDataset(test_file, tokenizer, max_length=512)
        sample = dataset[0]

        input_ids = sample["input_ids"].tolist()
        labels = sample["labels"].tolist()

        # è·å–ç‰¹æ®Š token IDs
        im_start_id = tokenizer.convert_tokens_to_ids(tokenizer.bos_token)
        im_end_id = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)
        pad_token_id = tokenizer.pad_token_id

        print("=" * 80)
        print("SFT Label Masking æµ‹è¯•")
        print("=" * 80)
        print(f"\nç‰¹æ®Š Token IDs:")
        print(f"  <|im_start|> (BOS): {im_start_id}")
        print(f"  <|im_end|> (EOS): {im_end_id}")
        print(f"  PAD: {pad_token_id}")

        # è§£æå¹¶æ˜¾ç¤ºæ¯ä¸ª block çš„ label è®¾ç½®
        print("\n" + "=" * 80)
        print("Token åºåˆ—åˆ†æï¼ˆåªæ˜¾ç¤ºé padding éƒ¨åˆ†ï¼‰:")
        print("=" * 80)

        # æ‰¾åˆ°ç¬¬ä¸€ä¸ª padding token çš„ä½ç½®
        try:
            first_pad_idx = input_ids.index(pad_token_id)
        except ValueError:
            first_pad_idx = len(input_ids)

        # åªåˆ†æé padding éƒ¨åˆ†
        input_ids_no_pad = input_ids[:first_pad_idx]
        labels_no_pad = labels[:first_pad_idx]

        i = 0
        block_num = 0

        while i < len(input_ids_no_pad):
            if input_ids_no_pad[i] == im_start_id:
                block_start = i
                i += 1

                # æ‰¾åˆ°å¯¹åº”çš„ <|im_end|>
                while i < len(input_ids_no_pad) and input_ids_no_pad[i] != im_end_id:
                    i += 1

                if i < len(input_ids_no_pad):
                    block_end = i + 1

                    # è§£ç è¿™ä¸ª block
                    block_tokens = input_ids_no_pad[block_start:block_end]
                    block_labels = labels_no_pad[block_start:block_end]
                    block_text = tokenizer.decode(block_tokens)

                    print(f"\nğŸ“¦ Block {block_num}:")
                    print(f"ä½ç½®: [{block_start}:{block_end}]")
                    print(f"å†…å®¹: {repr(block_text)}")

                    # ç»Ÿè®¡è¿™ä¸ª block ä¸­å“ªäº›ä½ç½®éœ€è¦è®¡ç®— loss
                    loss_positions = [
                        j for j, lbl in enumerate(block_labels) if lbl != -100
                    ]

                    if loss_positions:
                        print(f"âœ… è®¡ç®— Loss: æ˜¯")
                        print(
                            f"   Loss è¦†ç›–èŒƒå›´: ç›¸å¯¹ä½ç½® {loss_positions[0]} åˆ° {loss_positions[-1]}"
                        )

                        # æ˜¾ç¤ºå“ªäº›éƒ¨åˆ†è®¡ç®— loss
                        loss_start_abs = block_start + loss_positions[0]
                        loss_end_abs = block_start + loss_positions[-1] + 1
                        loss_tokens = input_ids_no_pad[loss_start_abs:loss_end_abs]
                        loss_text = tokenizer.decode(loss_tokens)
                        print(f"   Loss å†…å®¹: {repr(loss_text)}")

                        # æ£€æŸ¥æ˜¯å¦åŒ…å« <|im_end|>
                        if im_end_id in loss_tokens:
                            print(f"   âœ“ åŒ…å« <|im_end|> ç»“æŸç¬¦")
                    else:
                        print(f"âŒ è®¡ç®— Loss: å¦ (å…¨éƒ¨ masked)")

                    block_num += 1
                    i = block_end
            else:
                i += 1

        # è¯¦ç»†éªŒè¯
        print("\n" + "=" * 80)
        print("è¯¦ç»†éªŒè¯:")
        print("=" * 80)

        # éªŒè¯ç‚¹ 1: System å’Œ User æ¶ˆæ¯åº”è¯¥å…¨éƒ¨è¢« mask
        print("\nâœ“ æ£€æŸ¥ 1: System å’Œ User æ¶ˆæ¯åº”è¯¥å…¨éƒ¨è¢« masked (-100)")

        # éªŒè¯ç‚¹ 2: Assistant æ¶ˆæ¯åº”è¯¥åªåœ¨å®é™…å†…å®¹éƒ¨åˆ†è®¡ç®— loss
        print("âœ“ æ£€æŸ¥ 2: Assistant æ¶ˆæ¯åªå¯¹å®é™…å›å¤å†…å®¹è®¡ç®— loss")
        print("          (ä¸åŒ…æ‹¬ '<|im_start|>assistant\\n' header)")

        # éªŒè¯ç‚¹ 3: <|im_end|> åº”è¯¥è¢«åŒ…å«åœ¨ loss è®¡ç®—ä¸­
        print("âœ“ æ£€æŸ¥ 3: <|im_end|> åº”è¯¥åŒ…å«åœ¨ loss è®¡ç®—ä¸­")

        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_tokens = len(input_ids_no_pad)
        masked_tokens = sum(1 for lbl in labels_no_pad if lbl == -100)
        loss_tokens = total_tokens - masked_tokens

        print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  æ€» token æ•° (ä¸å« padding): {total_tokens}")
        print(
            f"  Masked tokens (-100): {masked_tokens} ({masked_tokens/total_tokens*100:.1f}%)"
        )
        print(
            f"  è®¡ç®— Loss çš„ tokens: {loss_tokens} ({loss_tokens/total_tokens*100:.1f}%)"
        )

        print("\n" + "=" * 80)
        print("æµ‹è¯•å®Œæˆï¼")
        print("=" * 80)

    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(test_file):
            os.remove(test_file)


if __name__ == "__main__":
    test_label_masking()
